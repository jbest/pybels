{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10d5c55b-a50b-4051-8c93-320ba4fa31d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Loads a CSV DwC occurrence file that has been augmented with BELS locality strings\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Loads a CSV DwC occurrence file that has been augmented with BELS locality strings\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c61251ce-f28e-4241-b613-cf19bff315a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "\n",
    "# switch to suppress CSV writing (which is slow)\n",
    "write_csv = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f7962e2-b7e9-4fce-9e68-abf3139f9ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_occ = pd.read_csv('TORCH-Denton-9f30f07b-e694-4f79-83c3-54fafaa98727.csv', low_memory=False)\n",
    "#df_occ = pd.read_csv('r_dale_thomas_texas_bels.csv', low_memory=False)\n",
    "#df_occ = pd.read_csv('r-dale-thomas-all-seinet-texas-bels.csv', low_memory=False)\n",
    "#df_occ = pd.read_csv('torch_bels_locs.csv', low_memory=False, sep='\\t')\n",
    "# BRIT UT test\n",
    "#df_occ = pd.read_csv('data_tsv/torch_bels_locs.tsv', low_memory=False, sep='\\t')\n",
    "df_occ = pd.read_csv('BELS_test_XXX.tsv', low_memory=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3de6262-33d4-4dc4-a292-92b759b36714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6181, 105)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_occ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfc2b0e4-a79b-4ffd-8635-6baa27a9d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e63d2a7-cb74-4062-83d8-62f899bfd4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_occ.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb4b35d4-dd6d-4de5-a3fa-5809c4638bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant colums\n",
    "# NOTE not all of these columns occur across all collections\n",
    "drop_columns= ['higherClassification','kingdom','phylum','class','order','identificationReferences','identificationRemarks','taxonRemarks','identificationQualifier','typeStatus','fieldNumber','eventID','informationWithheld','dataGeneralizations','dynamicProperties','associatedSequences','associatedTaxa','reproductiveCondition','establishmentMeans','lifeStage','sex','individualCount','samplingProtocol','preparations','continent','waterBody','islandGroup','island','rights','rightsHolder','accessRights','recordID','type','license','bibliographicCitation','datasetName','fieldNotes','countryCode','nomenclaturalCode','nomenclaturalStatus','associatedMedia','higherGeography','institutionID','georeferencedDate','datasetID','occurrenceStatus','verbatimLocality','organismID','previousIdentifications','eventTime','eventRemarks','locationAccordingTo','verbatimCoordinateSystem','footprintWKT','earliestEonOrLowestEonothem','earliestEraOrLowestErathem','earliestPeriodOrLowestSystem','earliestEpochOrLowestSeries','earliestAgeOrLowestStage','group','formation','member','identificationVerificationStatus','scientificNameID']\n",
    "df_occ = df_occ.drop(columns=drop_columns, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8ce3fd1-65a5-4901-b8a6-c7a74eb54116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6181, 75)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_occ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34cca590-0b35-4c4f-9a15-52e100471905",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#create a sample\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_occ_sample \u001b[38;5;241m=\u001b[39m \u001b[43mdf_occ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/jbest/data3/BRIT_git/pybels/env/lib/python3.12/site-packages/pandas/core/generic.py:6118\u001b[0m, in \u001b[0;36mNDFrame.sample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[1;32m   6115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   6116\u001b[0m     weights \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mpreprocess_weights(\u001b[38;5;28mself\u001b[39m, weights, axis)\n\u001b[0;32m-> 6118\u001b[0m sampled_indices \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6119\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(sampled_indices, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   6121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n",
      "File \u001b[0;32m/media/jbest/data3/BRIT_git/pybels/env/lib/python3.12/site-packages/pandas/core/sample.py:152\u001b[0m, in \u001b[0;36msample\u001b[0;34m(obj_len, size, replace, weights, random_state)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weights: weights sum to zero\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\n\u001b[1;32m    153\u001b[0m     np\u001b[38;5;241m.\u001b[39mintp, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    154\u001b[0m )\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:1020\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "#create a sample\n",
    "#df_occ_sample = df_occ.sample(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a340090b-7a65-44f0-9cc4-a2ed09c149f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 62)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_occ_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c308ea2a-467b-4a83-a0e5-91e974a40eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_occ_sample.to_csv('torch_bels_locs_SAMPLE.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ad9ef1-d969-420a-bf3a-41a760f6faf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicate records\n",
    "\n",
    "#bels_matchwithcoords\n",
    "df_matches = df_occ[df_occ['bels_location_string'].duplicated(keep=False)]\n",
    "#df_matches = df_denton[df_denton.bels_matchwithcoords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bf6483c-299c-4640-8a3d-65af99aaebf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4543, 75)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_matches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e571447d-c5a0-4f3e-a838-175992831d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(df_matches.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3e272de-5508-4d96-8a46-4dbeb4bf53fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add location ID\n",
    "# first create a copy to avoid copy of a slice of a DF\n",
    "df_matches = df_matches.copy()\n",
    "# https://stackoverflow.com/a/51110197 or https://stackoverflow.com/a/51110205\n",
    "df_matches['loc_id'] = df_matches.groupby(['bels_location_string']).ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74ff8d1a-59f4-486b-9c48-00d347f4656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dup loc count column for the size of each location cluster (number of duplicate locations)\n",
    "#https://stackoverflow.com/a/46768694\n",
    "#TODO - better name and documentation?\n",
    "# This is \n",
    "df_matches['dup_loc_count'] = df_matches.groupby(['bels_location_string']).transform('size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d5814dd-3986-46a7-b550-264c05a9e797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original DataFrame:\n",
      "(4543, 77)\n",
      "\n",
      "Filtered DataFrame (locations with at least one set of coordinates):\n",
      "(2201, 78)\n"
     ]
    }
   ],
   "source": [
    "# find groups that have at least one georeference\n",
    "# from https://claude.ai/chat/f9eab37d-6cc0-459c-9d24-fd78d6152d10\n",
    "#import pandas as pd\n",
    "\n",
    "def filter_locations_with_coordinates(df):\n",
    "    \"\"\"\n",
    "    Filter location groups based on having at least one record with coordinates.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input DataFrame with location_string and coordinates columns\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Filtered DataFrame containing all records from location groups\n",
    "                     that have at least one set of coordinates\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the number of coordinates for each location group\n",
    "    coord_counts = df[df['decimalLatitude'].notna() & (df['decimalLatitude'] != '')].groupby('bels_location_string').size()\n",
    "\n",
    "    # Find locations that have at least one non-empty coordinates value\n",
    "    # This handles various types of \"empty\" values (None, NaN, empty string)\n",
    "    #locations_with_coords = df[df['decimalLatitude'].notna() & (df['decimalLatitude'] != '')]['bels_location_string'].unique()\n",
    "    \n",
    "    # Find locations that have at least one non-empty coordinates value\n",
    "    # changed approach now that we have coord_counts\n",
    "    locations_with_coords = coord_counts.index\n",
    "    \n",
    "    # Filter the original DataFrame to keep all records from matching locations\n",
    "    #filtered_df = df[df['bels_location_string'].isin(locations_with_coords)]\n",
    "    # (New approach) Filter the original DataFrame to keep all records from matching locations\n",
    "    filtered_df = df[df['bels_location_string'].isin(locations_with_coords)].copy()\n",
    "\n",
    "    # Add the coordinate count for each location\n",
    "    filtered_df['recs_w_geo_count'] = filtered_df['bels_location_string'].map(coord_counts)\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# Filter locations that have at least one set of coordinates\n",
    "#result = filter_locations_with_coordinates(df)\n",
    "df_matches_wgeo = filter_locations_with_coordinates(df_matches)\n",
    "\n",
    "print(\"\\nOriginal DataFrame:\")\n",
    "print(df_matches.shape)\n",
    "print(\"\\nFiltered DataFrame (locations with at least one set of coordinates):\")\n",
    "print(df_matches_wgeo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a3897b5-0b36-4f61-8501-02e11798b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to TSV\n",
    "df_matches_wgeo.to_csv('TORCH_bels_matches_w_geo.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0abbfac-1745-4daf-93d1-a062f4767ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if write_csv:\n",
    "#    df_matches.to_csv('TORCH_bels_matches_loc_id.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c25d8841-c31c-4b0a-be13-b7505cd3b7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original DataFrame:\n",
      "(4543, 77)\n",
      "\n",
      "Filtered DataFrame (locations with at least one set of coordinates):\n",
      "(2342, 77)\n"
     ]
    }
   ],
   "source": [
    "# find groups that have at least one georeference\n",
    "# from https://claude.ai/chat/f9eab37d-6cc0-459c-9d24-fd78d6152d10\n",
    "#import pandas as pd\n",
    "\n",
    "def filter_locations_without_coordinates(df):\n",
    "    \"\"\"\n",
    "    Filter location groups based on having NO coordinates in any record.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input DataFrame with location_string and coordinates columns\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Filtered DataFrame containing all records from location groups\n",
    "                     that have no coordinates at all\n",
    "    \"\"\"\n",
    "    # Group by location_string and check if ALL coordinates are empty for that group\n",
    "    locations_without_coords = df.groupby('bels_location_string').agg({\n",
    "        'decimalLatitude': lambda x: all(pd.isna(x) | (x == ''))\n",
    "    })\n",
    "    \n",
    "    # Get the location strings where the condition is True\n",
    "    locations_no_coords = locations_without_coords[\n",
    "        locations_without_coords['decimalLatitude']\n",
    "    ].index\n",
    "    \n",
    "    # Filter the original DataFrame to keep all records from matching locations\n",
    "    filtered_df = df[df['bels_location_string'].isin(locations_no_coords)]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "#df = pd.DataFrame(data)\n",
    "\n",
    "# Filter locations that have at least one set of coordinates\n",
    "#result = filter_locations_with_coordinates(df)\n",
    "df_matches_wo_geo = filter_locations_without_coordinates(df_matches)\n",
    "\n",
    "print(\"\\nOriginal DataFrame:\")\n",
    "print(df_matches.shape)\n",
    "print(\"\\nFiltered DataFrame (locations with at least one set of coordinates):\")\n",
    "print(df_matches_wo_geo.shape)\n",
    "\n",
    "# Print summary\n",
    "#print(\"\\nSummary of locations without any coordinates:\")\n",
    "#for loc in df_matches_wo_geo['bels_location_string'].unique():\n",
    "#    print(f\"- {loc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cb04c91-7d17-429b-8be1-df15633e6935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to TSV\n",
    "df_matches_wo_geo.to_csv('TORCH_bels_matches_wo_geo.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1f12a70-8997-407c-84a7-cdc498679433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD method\n",
    "# find location group records lacking geo\n",
    "#bels_decimallatitude\n",
    "#df_nogeo = df_matches[df_matches['decimalLatitude'].isna()]\n",
    "#TODO add loc ID - https://stackoverflow.com/a/51110205\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12da5e88-0541-430d-9ddb-d4f5d3311d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all dups with and without geocoords\n",
    "dup_loc_count = df_matches.pivot_table(index = ['bels_location_string'], aggfunc ='size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4ce0341-1a9a-4f7e-82ce-87ed8826974f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(726,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_loc_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cb76bf0-a0a0-4f2c-bb18-dd07166b75f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if write_csv:\n",
    "    dup_loc_count.to_csv('TORCH_dup_loc_count.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c297e01a-fe76-4f38-982c-a33f8b181783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dups with no goecoords\n",
    "#dups = df.pivot_table(index = ['Course'], aggfunc ='size') \n",
    "#dup_loc_count_no_geo = df_nogeo.pivot_table(index = ['bels_location_string'], aggfunc ='size')\n",
    "#df_matches_wo_geo\n",
    "dup_loc_count_no_geo = df_matches_wo_geo.pivot_table(index = ['bels_location_string'], aggfunc ='size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cddad54e-b714-4487-abfe-bb76216f750a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(335,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_loc_count_no_geo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6096a3eb-7b9a-450b-919b-ea69cc435d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "if write_csv:\n",
    "    dup_loc_count_no_geo.to_csv('TORCH_dup_loc_count_wo_geo.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ded16c5-6956-4691-a2fb-403b46a618b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dups with goecoords\n",
    "#dup_loc_count_geo = df_geo.pivot_table(index = ['bels_location_string'], aggfunc ='size')\n",
    "dup_loc_count_geo = df_matches_wgeo.pivot_table(index = ['bels_location_string'], aggfunc ='size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fca09c5-2be5-42b5-84dc-e3b4724103d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(391,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_loc_count_geo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "750a98fa-8c1a-4d46-b506-23de30948f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "if write_csv:\n",
    "    dup_loc_count_geo.to_csv('TORCH_dup_loc_count_w_geo.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2102162a-07b1-49e4-8d03-3cfb21b56d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dup_loc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3eb80d43-99c4-47ab-b3c5-89aa3730b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter records that mention no additional locality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a51d20d8-7c48-4873-b085-5fb95f696de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(df_matches.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "384fa1f0-2d25-4101-a0db-b16b5d619d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ChatGPT\n",
    "#https://chat.openai.com/share/21c561bb-db43-47ee-980a-f88078b8c78b\n",
    "# Identify duplicate values in a specific column\n",
    "#\n",
    "#duplicates = df_matches['bels_matchwithcoords'].duplicated(keep=False)\n",
    "#duplicates = df_matches['bels_location_string'].duplicated(keep=False)\n",
    "\n",
    "# Count the number of duplicates for each value\n",
    "#duplicate_counts = duplicates.groupby(df_matches['bels_matchwithcoords']).sum()\n",
    "#duplicate_counts = duplicates.groupby(df_matches['bels_location_string']).sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7dfb05e9-8fef-4e99-9df0-76f4827c3116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note needed, done above\n",
    "# Add a new column to the DataFrame with the number of duplicates\n",
    "#df_matches['duplicate_count'] = df_matches['bels_location_string'].map(duplicate_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "19147101-54d3-4b0c-a204-c8b313272b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicate_counts.sort_index().cumsum().plot()\n",
    "#duplicate_counts.set_index('duplicate_count')\n",
    "#duplicate_counts.sort_index().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a6524411-e264-4bc5-b7c6-224f71fac759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_matches[['duplicate_count']] = df_matches[['duplicate_count']].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "374b07e4-c23b-433b-a1f7-19d875a141a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_matches.set_index('duplicate_count').cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "610e3750-edc4-4845-ad52-fcc9ec5b7d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Load ids of elite georeferencers - EG\n",
    "df_georeferencers = pd.read_csv('TORCH-georeferencers_tested.csv', low_memory=False)\n",
    "\n",
    "#find records with geo that have been done by EGs\n",
    "#find records without geo that match those done by EGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "73c7b402-8881-40ed-9880-b7df339560ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter records that have been georeferenced by 'elite' vetted georeferencers\n",
    "df_by_egeo = df_matches[df_matches['georeferencedBy'].isin(df_georeferencers['Username'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8a20b4b7-5f87-4079-b655-8a7ec03a2c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30372, 64)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_by_egeo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fd596f17-57eb-4a19-a63c-4431cfd38b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if write_csv:\n",
    "    df_by_egeo.to_csv('TORCH_df_by_egeo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ff24581b-bf33-433c-b82c-afb2cf2a7a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find locations that match loc_id of those done by EG\n",
    "df_loc_match_by_egeo = df_matches[df_matches['loc_id'].isin(df_by_egeo['loc_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "fce4be80-e832-4518-8f94-23325340bb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45211, 64)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_loc_match_by_egeo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d0aa603b-dfe5-453d-9b6e-1c8490dddf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find matching locs that have not been georeffed\n",
    "df_loc_match_by_egeo_nogeo = df_loc_match_by_egeo[df_loc_match_by_egeo['decimalLatitude'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "508aa2c4-fff6-41ed-808e-82e83b8b5a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9424, 64)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_loc_match_by_egeo_nogeo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "23b82a18-c0ed-4a1d-a1e0-9067ffdcadaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find matching locs that have been georeffed\n",
    "df_loc_match_by_egeo_w_geo = df_loc_match_by_egeo[df_loc_match_by_egeo['decimalLatitude'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "58e71aee-8719-4c9d-bbdc-bdf9712ede5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35787, 64)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_loc_match_by_egeo_w_geo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5ec847ac-74fa-4d42-ba32-cd25bcaa3ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208011, 64)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nogeo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "947c47de-99ed-4449-83c7-533c00b5d059",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_nogeo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# find dup locs without geo but have other dups that have geo\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_nogeo_with_geolocdups \u001b[38;5;241m=\u001b[39m \u001b[43mdf_nogeo\u001b[49m[df_nogeo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloc_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(df_geo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloc_id\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#df_matches_wo_geo\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#df_nogeo_with_geolocdups = df_matches_wo_geo[df_matches_wo_geo['loc_id'].isin(df_geo['loc_id'])]\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_nogeo' is not defined"
     ]
    }
   ],
   "source": [
    "# find dup locs without geo but have other dups that have geo\n",
    "df_nogeo_with_geolocdups = df_nogeo[df_nogeo['loc_id'].isin(df_geo['loc_id'])]\n",
    "#df_matches_wo_geo\n",
    "#df_nogeo_with_geolocdups = df_matches_wo_geo[df_matches_wo_geo['loc_id'].isin(df_geo['loc_id'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "69e96a46-e482-47b0-aa4c-6d79c83aa71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54747, 64)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nogeo_with_geolocdups.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "329b18c2-fce5-4c4e-9f63-5abfe55f8d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "if write_csv:\n",
    "    df_nogeo_with_geolocdups.to_csv('TORCH_nogeo_with_geolocdups.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c46eb5c5-5d7b-4a1c-962b-ecf33ebcfa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2eda1b94-a599-44da-bf0a-58106ba9dd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop records with only the state name in bels string\n",
    "#df_nogeo_with_geolocdups_nostates = df_nogeo_with_geolocdups[df_nogeo_with_geolocdups[column1] != df[column2]]\n",
    "\n",
    "#df_nogeo_with_geolocdups_nostates = df_nogeo_with_geolocdups[~df_nogeo_with_geolocdups.apply(lambda row: row['stateProvince'].lower() == row['bels_location_string'].lower(), axis=1)]\n",
    "#df_matches_wo_geo\n",
    "df_wo_geo_nostates = df_matches_wo_geo[~df_matches_wo_geo.apply(lambda row: row['stateProvince'].lower() == row['bels_location_string'].lower(), axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7348adfa-dbbb-40d2-9cb1-4c8fecb0a7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(309918, 64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_nogeo_with_geolocdups_nostates.shape\n",
    "df_wo_geo_nostates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2d147df-6d7d-488e-b4eb-a14780cd6b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if write_csv:\n",
    "    df_wo_geo_nostates.to_csv('TORCH_wo_geo_nostates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "367ad0fd-6c62-424b-85b5-f2cbefc031bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove state + county matches\n",
    "#df_nogeo_with_geolocdups_nostatecounty = df_nogeo_with_geolocdups_nostates[~df_nogeo_with_geolocdups_nostates.apply(lambda row: (str(row['stateProvince']) + str(row['county'])).lower() == row['bels_location_string'].lower(), axis=1)]\n",
    "#df_wo_geo_nostates\n",
    "df_wo_geo_nostatecounty = df_wo_geo_nostates[~df_wo_geo_nostates.apply(lambda row: (str(row['stateProvince']) + str(row['county'])).lower() == row['bels_location_string'].lower(), axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c3e6eec-044f-4d02-9535-2537c2f9b7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(277948, 64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_nogeo_with_geolocdups_nostatecounty.shape\n",
    "df_wo_geo_nostatecounty.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40985a08-777c-4d6a-9bf5-0225c7c5dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if write_csv:\n",
    "    df_wo_geo_nostatecounty.to_csv('TORCH_wo_geo_nostatecounty.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b43caf7f-7d41-4d90-b036-d73b7e13793c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list[Index(['id', 'institutionCode', 'collectionCode', 'ownerInstitutionCode',\n",
       "       'collectionID', 'basisOfRecord', 'occurrenceID', 'catalogNumber',\n",
       "       'otherCatalogNumbers', 'family', 'scientificName', 'taxonID',\n",
       "       'scientificNameAuthorship', 'genus', 'subgenus', 'specificEpithet',\n",
       "       'verbatimTaxonRank', 'infraspecificEpithet', 'taxonRank',\n",
       "       'identifiedBy', 'dateIdentified', 'recordedBy', 'recordNumber',\n",
       "       'eventDate', 'year', 'month', 'day', 'startDayOfYear', 'endDayOfYear',\n",
       "       'verbatimEventDate', 'occurrenceRemarks', 'habitat',\n",
       "       'associatedOccurrences', 'locationID', 'country', 'stateProvince',\n",
       "       'county', 'municipality', 'locality', 'locationRemarks',\n",
       "       'decimalLatitude', 'decimalLongitude', 'geodeticDatum',\n",
       "       'coordinateUncertaintyInMeters', 'verbatimCoordinates',\n",
       "       'georeferencedBy', 'georeferenceProtocol', 'georeferenceSources',\n",
       "       'georeferenceVerificationStatus', 'georeferenceRemarks',\n",
       "       'minimumElevationInMeters', 'maximumElevationInMeters',\n",
       "       'minimumDepthInMeters', 'maximumDepthInMeters', 'verbatimDepth',\n",
       "       'verbatimElevation', 'disposition', 'language', 'recordEnteredBy',\n",
       "       'modified', 'references', 'bels_location_string', 'loc_id',\n",
       "       'dup_loc_count'],\n",
       "      dtype='object')]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list[df_nogeo_with_geolocdups_nostatecounty.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d87d2842-c8f1-4db4-af47-eb0bdfedc2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add counts by county\n",
    "#df_nogeo_with_geolocdups_nostatecounty\n",
    "#df_wo_geo_nostatecounty\n",
    "county_summary = df_wo_geo_nostatecounty.groupby(['stateProvince','county']).agg(\n",
    "    total_locations=('loc_id', 'count'),\n",
    "    #sum_id_count=('dup_loc_count', 'sum'),\n",
    "    unique_locations=('loc_id', 'nunique'),\n",
    "    dup_loc_count=('dup_loc_count', 'max')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e3492d3-b98b-4c57-bd93-c631ef416ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>total_locations</th>\n",
       "      <th>unique_locations</th>\n",
       "      <th>dup_loc_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stateProvince</th>\n",
       "      <th>county</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Oklahoma</th>\n",
       "      <th>Adair</th>\n",
       "      <td>1381</td>\n",
       "      <td>295</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alfalfa</th>\n",
       "      <td>265</td>\n",
       "      <td>72</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Angelina</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Atoka</th>\n",
       "      <td>1227</td>\n",
       "      <td>254</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Atoka?</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Texas</th>\n",
       "      <th>not listed</th>\n",
       "      <td>807</td>\n",
       "      <td>53</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presidio</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starr</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walker</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>willacy</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>477 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          total_locations  unique_locations  dup_loc_count\n",
       "stateProvince county                                                      \n",
       "Oklahoma      Adair                  1381               295             57\n",
       "              Alfalfa                 265                72             18\n",
       "              Angelina                  2                 1              2\n",
       "              Atoka                  1227               254             53\n",
       "              Atoka?                    1                 1            107\n",
       "...                                   ...               ...            ...\n",
       "Texas         not listed              807                53            316\n",
       "              presidio                  1                 1              4\n",
       "              starr                     1                 1              2\n",
       "              walker                    2                 2              2\n",
       "              willacy                   1                 1              2\n",
       "\n",
       "[477 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "county_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ae40f95-8f04-486a-86f3-823977e0e861",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_summary.to_csv('TORCH_wo_geo_county_summary.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "fe03941c-0b9b-4fa4-b46e-8f3ec9319e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202546, 64)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returning to unfiltered DF for more comprehensive results\n",
    "\n",
    "#df_nogeo\n",
    "# remove states only\n",
    "df_nogeo_nostates = df_nogeo[~df_nogeo.apply(lambda row: row['stateProvince'].lower() == row['bels_location_string'].lower(), axis=1)]\n",
    "#remove state and county only\n",
    "df_nogeo_nostatecounty = df_nogeo_nostates[~df_nogeo_nostates.apply(lambda row: (str(row['stateProvince']) + str(row['county'])).lower() == row['bels_location_string'].lower(), axis=1)]\n",
    "\n",
    "df_nogeo_nostatecounty.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c5ee249b-9eb3-476e-9386-17243cbc2105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary with reset index:\n",
      "    stateProvince    county  unique_locations  loc_count\n",
      "0        Oklahoma     Adair               113        433\n",
      "1        Oklahoma   Alfalfa                29         80\n",
      "2        Oklahoma  Angelina                 1          2\n",
      "3        Oklahoma     Atoka               168        857\n",
      "4        Oklahoma    Beaver                30        147\n",
      "..            ...       ...               ...        ...\n",
      "345         Texas    Yoakum                 7         23\n",
      "346         Texas     Young                50        151\n",
      "347         Texas    Zapata               161        439\n",
      "348         Texas    Zavala                47        165\n",
      "349         Texas  mitchell                 1          1\n",
      "\n",
      "[350 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "county_summary_nogeo = df_nogeo_nostatecounty.groupby(['stateProvince','county']).agg(\n",
    "    #total_locations=('loc_id', 'count'),\n",
    "    unique_locations=('loc_id', 'nunique'),\n",
    "    #sum_id_count=('dup_loc_count', 'sum')\n",
    "    loc_count=('loc_id', 'count')\n",
    ")\n",
    "# Reset the index to make 'state' and 'county' regular columns\n",
    "county_summary_nogeo = county_summary_nogeo.reset_index()\n",
    "print(\"\\nSummary with reset index:\")\n",
    "print(county_summary_nogeo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1e4466b5-0972-4417-bcbb-efd652fa04ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same process but only include BRIT records in Texas\n",
    "# returning to unfiltered DF for more comprehensive results\n",
    "\n",
    "#df_nogeo\n",
    "# Texas collections only\n",
    "df_torch_texas_nogeo = df_nogeo[df_nogeo['stateProvince'] == 'Texas']\n",
    "# remove states only\n",
    "df_torch_texas_nogeo_nostates = df_torch_texas_nogeo[~df_torch_texas_nogeo.apply(lambda row: row['stateProvince'].lower() == row['bels_location_string'].lower(), axis=1)]\n",
    "#remove state and county only\n",
    "df_torch_texas_nogeo_nostatecounty = df_torch_texas_nogeo_nostates[~df_torch_texas_nogeo_nostates.apply(lambda row: (str(row['stateProvince']) + str(row['county'])).lower() == row['bels_location_string'].lower(), axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ce440cd9-f6bc-422f-86e7-4a29d14bfc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175474, 64)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_torch_texas_nogeo_nostatecounty.shape  # old (106414, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "01c96b0f-7c08-4d48-b347-a67be046bea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72524, 64)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BRIT collections only\n",
    "df_torch_texas_brit_nogeo = df_torch_texas_nogeo_nostatecounty[(df_torch_texas_nogeo_nostatecounty['institutionCode'] == 'BRIT') | (df_torch_texas_nogeo_nostatecounty['institutionCode'] == 'VDB')]\n",
    "\n",
    "df_torch_texas_brit_nogeo.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "762f87f5-c5df-4cea-9d35-3ee48acc00d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_torch_texas_brit_nogeo.to_csv('torch_texas_brit_nogeo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "bdd19ed6-903d-496d-b4d5-b2713190c048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary with reset index:\n",
      "    stateProvince    county  unique_locations  loc_count  largest_dups\n",
      "0           Texas  Anderson               204        809            77\n",
      "1           Texas   Andrews                 3          4             4\n",
      "2           Texas  Angelina               136        421            70\n",
      "3           Texas   Aransas               175        725           249\n",
      "4           Texas    Archer                38        145            44\n",
      "..            ...       ...               ...        ...           ...\n",
      "225         Texas      Wise                79        221            68\n",
      "226         Texas      Wood               185        882            58\n",
      "227         Texas     Young                35         99            47\n",
      "228         Texas    Zapata               133        337            65\n",
      "229         Texas    Zavala                28         85            16\n",
      "\n",
      "[230 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Summarize BRIT Texas records\n",
    "brit_texas_county_summary_nogeo = df_torch_texas_brit_nogeo.groupby(['stateProvince','county']).agg(\n",
    "    #total_locations=('loc_id', 'count'),\n",
    "    unique_locations=('loc_id', 'nunique'),\n",
    "    #sum_id_count=('dup_loc_count', 'sum')\n",
    "    loc_count=('loc_id', 'count'),\n",
    "    largest_dups = ('dup_loc_count', 'max')\n",
    ")\n",
    "# Reset the index to make 'state' and 'county' regular columns\n",
    "brit_texas_county_summary_nogeo = brit_texas_county_summary_nogeo.reset_index()\n",
    "print(\"\\nSummary with reset index:\")\n",
    "print(brit_texas_county_summary_nogeo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e795c04b-07d2-4a2f-9ca5-a6df54e7f533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_torch_texas_nogeo_nostatecounty.shape\n",
    "# export BRIT Texas summary\n",
    "brit_texas_county_summary_nogeo.to_csv('TORCH_brit_texas_county_summary_nogeo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "8f25498e-64e3-4acb-b12a-23442e4647cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#county_summary_nogeo.to_csv('TORCH_county_summary_nogeo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "5d4c5c96-b9be-49c6-addf-455b1b9b1f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 64)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sampling individual counties\n",
    "#df_torch_texas_brit_nogeo\n",
    "# df_torch_texas_brit_nogeo = df_torch_texas_nogeo_nostatecounty[(df_torch_texas_nogeo_nostatecounty['institutionCode'] == 'BRIT') | (df_torch_texas_nogeo_nostatecounty['institutionCode'] == 'VDB')]\n",
    "\n",
    "df_brit_young_nogeo = df_torch_texas_brit_nogeo[df_torch_texas_brit_nogeo['county'] == 'Young']\n",
    "\n",
    "df_brit_young_nogeo.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0c9e3c56-9170-4b39-ac84-029c7641bf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brit_young_nogeo.to_csv('brit_young_nogeo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6294df-2e4a-4c0e-9f37-3bf958f22920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
